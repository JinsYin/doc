# Ceph 客户端

## 服务接口

Ceph 客户端提供了许多服务接口：

| Ceph 客户端        | 服务接口                                                                                                                                                                                                                           |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 块设备（RBD）      | * 提供支持 `快照`、 `克隆`、`大小可调` 等功能的块设备 <br> * Ceph 在整个集群中对块设备进行 `条带化`（stripe）以获得高性能 <br> * Ceph 支持内核对象（KO），以及直接使用 librbd 的 QEMU Hypervisor（避免了虚拟化系统的内核对象开销） |
| 对象存储（RGW）    | 提供与 Amazon S3 和 OpenStack Swift 兼容的 RESTful API 接口                                                                                                                                                                        |
| 文件系统（CephFS） | 提供一个 POSIX 兼容的文件系统，可以挂载或作为 FUSE（Filesystem in Userspace）使用                                                                                                                                                  |

高级体系结构：

```c
+----------------+    +----------------+    +----------------+
|  Block Device  |    | Object Storage |    |     CephFS     |
+----------------+    +----------------+    +----------------+

+----------------+    +----------------+    +----------------+
|     librbd     |    |     librgw     |    |    libcephfs   |
+----------------+    +----------------+    +----------------+

+------------------------------------------------------------+
|           Ceph Storage Cluster Protocol (librados)         |
+------------------------------------------------------------+

+----------------+    +----------------+    +----------------+
|      OSDs      |    |      MDSs      |    |     Monitors   |
+----------------+    +----------------+    +----------------+
```

## Ceph 对象存储

* Ceph 对象存储守护进程 radosgw 是一种 FastCGI 服务，它提供 RESTful HTTP API 来存储对象和元数据
* Radosgw 在 Ceph 存储集群上层使用了自己的数据格式，并维护自己的用户数据库、身份验证和访问控制
* RADOS Gateway 使用统一命名空间，意味着可以使用兼容 OpenStack Swift 或 Amonzon S3 的 API（例如，使用 S3 写，然后使用 Swift 读）

| 对象              | 比较                                                                                                                                                     |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Ceph 存储集群对象 | 使用术语 _object_ 描述存储的数据                                                                                                                         |
| S3/Swift 对象     | * `Ceph 对象存储的对象` 映射为 `Ceph 存储集群的对象` <br> * S3/Swift 对象不一定以 1:1 的形式与存储在 `Ceph 存储集群` 中的对象相对应，也可能是 1:n 的形式 |

## Ceph 块设备

* Ceph 块设备在 Ceph 存储集群中的多个对象上对块设备

## Ceph 文件系统

* Ceph 文件系统（CephFS）提供符合 POSIX 标准的文件系统作为一种服务，它位于基于对象的 Ceph 存储集群之上
* CephFS 文件被映射到 Ceph 存储在 `Ceph 存储集群` 中的对象
* Ceph 客户端将 CephFS 文件系统挂载为内核对象或 FUSE（Filesystem in Userspace）

```c
+-------------------------+        +-------------------------+
|  CephFS Kernel Object   |        |       CephFS FUSE        |
+-------------------------+        +-------------------------+

+------------------------------------------------------------+
|                  CephFS Library (libcephfs)                |
+------------------------------------------------------------+

+------------------------------------------------------------+
|           Ceph Storage Cluster Protocol (librados)         |
+------------------------------------------------------------+

+----------------+    +----------------+    +----------------+
|      OSDs      |    |      MDSs      |    |     Monitors   |
+----------------+    +----------------+    +----------------+
```

* MDS 的目的是将所有文件系统元数据（目录、文件所有权、访问模式等）存储在元数据驻留在内存中的高可用 Ceph 元数据服务器中

* CephFS 将元数据和数据分离，将元数据存储在 MDS 中，并将文件数据存储在 Ceph 存储集群中的一个或多个对象中

ceph-mds 可以作为单个进程运行，也可以分发到多个物理机器，以实现高可用性和可扩展性。

* 高可用性：额外的 ceph-mds 实例可以处于 _standby_ 状态，当 _active_ ceph-mds 出错时随时接管其职责
* 可扩展性：多个 ceph-mds 实例可以使用 _active_ 状态的，它们会将目录树拆分成子树（以及单个繁忙目录的分片），有效平衡所有 _active_ 服务器之间的负载
* _standby_ 和 _active_ 可以进行组合，例如，为了可扩展性运行 3 个 _active_ ceph-mds 实例，为了高可用运行 1 个 _standby_ 实例
